{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Data Extraction from Binance\n",
    "- by Chee-Foong\n",
    "- on May 2020\n",
    "\n",
    "Adapted from this post by **Peter Nistrup**.  Thank you for sharing.\n",
    "\n",
    "https://medium.com/swlh/retrieving-full-historical-data-for-every-cryptocurrency-on-binance-bitmex-using-the-python-apis-27b47fd8137f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-binance\n",
      "  Downloading python_binance-0.7.5-py2.py3-none-any.whl (29 kB)\n",
      "Collecting cryptography\n",
      "  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 984 kB/s eta 0:00:01     |████▌                           | 378 kB 984 kB/s eta 0:00:03     |██████                          | 512 kB 984 kB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from python-binance) (2.22.0)\n",
      "Collecting pyOpenSSL\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/site-packages (from python-binance) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/site-packages (from python-binance) (1.25.8)\n",
      "Collecting dateparser\n",
      "  Downloading dateparser-0.7.4-py2.py3-none-any.whl (353 kB)\n",
      "\u001b[K     |████████████████████████████████| 353 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from python-binance) (1.14.0)\n",
      "Collecting autobahn\n",
      "  Downloading autobahn-20.4.3-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting service-identity\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting Twisted\n",
      "  Downloading Twisted-20.3.0.tar.bz2 (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 824 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cffi!=1.11.3,>=1.8\n",
      "  Downloading cffi-1.14.0-cp38-cp38-manylinux1_x86_64.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests->python-binance) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->python-binance) (2.8)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/site-packages (from dateparser->python-binance) (2019.3)\n",
      "Collecting regex!=2019.02.19\n",
      "  Downloading regex-2020.5.14-cp38-cp38-manylinux2010_x86_64.whl (689 kB)\n",
      "\u001b[K     |████████████████████████████████| 689 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzlocal\n",
      "  Downloading tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/site-packages (from dateparser->python-binance) (2.8.1)\n",
      "Collecting txaio>=20.3.1\n",
      "  Downloading txaio-20.4.1-py2.py3-none-any.whl (30 kB)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.8/site-packages (from service-identity->python-binance) (19.3.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.interface>=4.4.2\n",
      "  Downloading zope.interface-5.1.0-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Using cached incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-19.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 180 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from zope.interface>=4.4.2->Twisted->python-binance) (46.4.0)\n",
      "Building wheels for collected packages: Twisted\n",
      "  Building wheel for Twisted (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Twisted: filename=Twisted-20.3.0-cp38-cp38-linux_x86_64.whl size=3087612 sha256=2bc02c2edcb825e6929d89afd8eeb3627ced848ae0f5fc208916ce28b481b244\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/36/1b/99fe6d339e1559e421556c69ad7bc8c869145e86a756c403f4\n",
      "Successfully built Twisted\n",
      "Installing collected packages: pycparser, cffi, cryptography, pyOpenSSL, regex, tzlocal, dateparser, txaio, autobahn, pyasn1, pyasn1-modules, service-identity, zope.interface, constantly, incremental, Automat, hyperlink, PyHamcrest, Twisted, python-binance\n",
      "Successfully installed Automat-20.2.0 PyHamcrest-2.0.2 Twisted-20.3.0 autobahn-20.4.3 cffi-1.14.0 constantly-15.1.0 cryptography-2.9.2 dateparser-0.7.4 hyperlink-19.0.0 incremental-17.5.0 pyOpenSSL-19.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 python-binance-0.7.5 regex-2020.5.14 service-identity-18.1.0 txaio-20.4.1 tzlocal-2.1 zope.interface-5.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-binance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "from tqdm import tqdm_notebook #(Optional, used for progress-bars)\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a Binance API key-secret pair to access the data and update the key-secret pair in the json file: **../binance/api.json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../binance/api.json', 'r') as f:\n",
    "    api = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data folder to store all raw csv files downloaded from Binance.  These raw files are important to determine what data is already downloaded and hence need to download from Binance again.  Downloading full data from Binance may take a long time.\n",
    "\n",
    "Set the path name for the data folder in **data_folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "\n",
    "### API\n",
    "binance_api_key = api['key']       #Enter your own API-key here\n",
    "binance_api_secret = api['secret'] #Enter your own API-secret here\n",
    "\n",
    "### CONSTANTS\n",
    "binsizes = {\"1m\": 1, \"5m\": 5, \"1h\": 60, \"1d\": 1440}\n",
    "binance_client = Client(api_key=binance_api_key, api_secret=binance_api_secret)\n",
    "data_folder = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions adapted from **Peter Nistrup**'s write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS\n",
    "def minutes_of_new_data(symbol, kline_size, data, source):\n",
    "    if len(data) > 0:  \n",
    "        old = parser.parse(data[\"timestamp\"].iloc[-1])\n",
    "    elif source == \"binance\": \n",
    "        old = datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
    "\n",
    "    if source == \"binance\": \n",
    "        new = pd.to_datetime(binance_client.get_klines(symbol=symbol, interval=kline_size)[-1][0], unit='ms')\n",
    "\n",
    "    return old, new\n",
    "\n",
    "\n",
    "def get_all_binance(symbol, kline_size, save = False):\n",
    "    filename = data_folder + '%s-%s-data.csv' % (symbol, kline_size)\n",
    "    \n",
    "    if os.path.isfile(filename): \n",
    "        data_df = pd.read_csv(filename)\n",
    "    else: \n",
    "        data_df = pd.DataFrame()\n",
    "        \n",
    "    oldest_point, newest_point = minutes_of_new_data(symbol, kline_size, data_df, source = \"binance\")\n",
    "    delta_min = (newest_point - oldest_point).total_seconds()/60\n",
    "    available_data = math.ceil(delta_min/binsizes[kline_size])\n",
    "    \n",
    "    if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'): \n",
    "        print('Downloading all available %s data for %s. Be patient..!' % \n",
    "              (kline_size, symbol))\n",
    "    else: \n",
    "        print('Downloading %d minutes of new data available for %s, i.e. %d instances of %s data.' % \n",
    "              (delta_min, symbol, available_data, kline_size))\n",
    "        \n",
    "    klines = binance_client.get_historical_klines(symbol, kline_size, \n",
    "                                                  oldest_point.strftime(\"%d %b %Y %H:%M:%S\"), \n",
    "                                                  newest_point.strftime(\"%d %b %Y %H:%M:%S\"))\n",
    "    data = pd.DataFrame(klines, columns = ['timestamp', 'open', 'high', 'low', 'close', \n",
    "                                           'volume', 'close_time', 'quote_av', 'trades', \n",
    "                                           'tb_base_av', 'tb_quote_av', 'ignore' ])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
    "    \n",
    "    if len(data_df) > 0:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        data_df = data_df.append(temp_df)\n",
    "    else: \n",
    "        data_df = data\n",
    "        \n",
    "    data_df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    if save: \n",
    "        data_df.to_csv(filename)\n",
    "        \n",
    "    print('All caught up..!')\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a cryptocurrency symbols based in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = binance_client.get_exchange_info()\n",
    "\n",
    "symbols = info['symbols']\n",
    "coins = []\n",
    "others = []\n",
    "\n",
    "for i, symbol in enumerate(symbols):\n",
    "    s = symbol['symbol']\n",
    "    if ('USDT' in s) and (len(s) == 7) :\n",
    "#         print('{} - {}'.format(i, s))\n",
    "        coins.append(s)\n",
    "    elif ('USDT' in s):\n",
    "        others.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ff13d89b9b4b6fbfdc8812c0f95e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 573 minutes of new data available for BTCUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ETHUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for BNBUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 0 minutes of new data available for BCCUSDT, i.e. 0 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for NEOUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for LTCUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ADAUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for XRPUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for EOSUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for XLMUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ONTUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for TRXUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for ETCUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for ICXUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 0 minutes of new data available for VENUSDT, i.e. 0 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for VETUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for PAXUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for BTTUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ONGUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for HOTUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for ZILUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ZRXUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for FETUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for BATUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for XMRUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ZECUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for OMGUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for ENJUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for ONEUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for FTMUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for GTOUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for ERDUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for WINUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for COSUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for MTLUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for MFTUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for KEYUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for WANUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for FUNUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for CVCUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for CHZUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for XTZUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for RENUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for RVNUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for NKNUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for STXUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for RLCUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for MCOUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for BCHUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for FTTUSDT, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 572 minutes of new data available for BUSDTRY, i.e. 572 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for USDTTRY, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for USDTRUB, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for EURUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for OGNUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for TCTUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for WRXUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for BTSUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for LSKUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for BNTUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for LTOUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for MBLUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for USDTZAR, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for WTCUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for XZCUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for CHRUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "Downloading 573 minutes of new data available for GXSUSDT, i.e. 573 instances of 1m data.\n",
      "All caught up..!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange, notebook\n",
    "\n",
    "for symbol in notebook.tqdm(coins):\n",
    "    try:\n",
    "        get_all_binance(symbol, '1m', save = True)\n",
    "    except:\n",
    "        print('Skipping {}...'.format(symbol))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my analysis, I am only interested in CLOSE price.  Hence, I perform some data transformation and combined all the close prices into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6be301809e4dcf908fe64e576fbfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for symbol in notebook.tqdm(coins):\n",
    "    data = pd.read_csv(data_folder + symbol + '-1m-data.csv', parse_dates=True, index_col='timestamp')\n",
    "    data = pd.to_numeric(data.close).resample('1T').last()\n",
    "    data_list.append(data)\n",
    "    \n",
    "prices = reduce(lambda left, right: pd.merge(left, right, \n",
    "                                             left_on='timestamp', right_on='timestamp', \n",
    "                                             how='outer'), data_list)\n",
    "\n",
    "prices.columns = [symbols[0:3] for symbols in coins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the prices in a csv file for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.to_csv('../data/prices_backup.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading prices for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC</th>\n",
       "      <th>ETH</th>\n",
       "      <th>BNB</th>\n",
       "      <th>BCC</th>\n",
       "      <th>NEO</th>\n",
       "      <th>LTC</th>\n",
       "      <th>ADA</th>\n",
       "      <th>XRP</th>\n",
       "      <th>EOS</th>\n",
       "      <th>XLM</th>\n",
       "      <th>...</th>\n",
       "      <th>BTS</th>\n",
       "      <th>LSK</th>\n",
       "      <th>BNT</th>\n",
       "      <th>LTO</th>\n",
       "      <th>MBL</th>\n",
       "      <th>USD.2</th>\n",
       "      <th>WTC</th>\n",
       "      <th>XZC</th>\n",
       "      <th>CHR</th>\n",
       "      <th>GXS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:00:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>301.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:01:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>301.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:02:00</th>\n",
       "      <td>4280.56</td>\n",
       "      <td>300.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:03:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>300.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 04:04:00</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>301.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         BTC     ETH  BNB  BCC  NEO  LTC  ADA  XRP  EOS  XLM  \\\n",
       "timestamp                                                                      \n",
       "2017-08-17 04:00:00  4261.48  301.13  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2017-08-17 04:01:00  4261.48  301.13  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2017-08-17 04:02:00  4280.56  300.00  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2017-08-17 04:03:00  4261.48  300.00  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2017-08-17 04:04:00  4261.48  301.13  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                     ...  BTS  LSK  BNT  LTO  MBL  USD.2  WTC  XZC  CHR  GXS  \n",
       "timestamp            ...                                                      \n",
       "2017-08-17 04:00:00  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "2017-08-17 04:01:00  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "2017-08-17 04:02:00  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "2017-08-17 04:03:00  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "2017-08-17 04:04:00  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = pd.read_csv('../data/prices_backup.csv', parse_dates=True, index_col='timestamp')\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions written by Peter Nistrup made downloading prices from Binance a breeze.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
